{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import moviepy.editor as mped\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class AudioDataset(Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self, path, dataset = 'train', chunk_size = 4, chunk_step = 1/40, sr = 22050,\n",
    "                 frame_length = 1024, win_length = 1024, hop_length = 512, use_mel = True, mel_count = 128,\n",
    "                 keep_audio = True, use_wobble = True, wobble = 2):\n",
    "        self.path = path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_step = chunk_step\n",
    "        self.sr = sr\n",
    "        self.frame_length = frame_length\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.use_mel = use_mel\n",
    "        self.mel_count = mel_count\n",
    "        self.pos_audio = None\n",
    "        self.neg_audio = None\n",
    "        self.keep_audio = keep_audio\n",
    "        self.use_wobble = use_wobble\n",
    "        self.wobble = wobble\n",
    "        \n",
    "        def get_file(file):\n",
    "            video = mped.VideoFileClip(file)\n",
    "            audio = video.audio\n",
    "            audio_tensor = torch.Tensor(audio.to_soundarray(fps=self.sr, nbytes=2)).t()\n",
    "            video.close()\n",
    "            audio.close()\n",
    "            audio_tensor = torch.mean(audio_tensor, dim=0, keepdim=True) # Stereo to mono\n",
    "\n",
    "            return audio_tensor\n",
    "        \n",
    "        print('Loading audio into memory')\n",
    "        labels = ['Negative', 'Positive']\n",
    "        for label in labels:\n",
    "            for root, directories, files in os.walk(os.path.join(self.path, dataset.capitalize(), label), topdown=False):\n",
    "                for file in files:\n",
    "                    if file.endswith('.mp4'):\n",
    "                        try :\n",
    "                            if label == 'Positive':\n",
    "                                if self.pos_audio is None:\n",
    "                                    self.pos_audio = get_file(os.path.join(root, file))\n",
    "                                elif torch.is_tensor(self.pos_audio):\n",
    "                                    audio = get_file(os.path.join(root, file))\n",
    "                                    self.pos_audio = torch.cat((self.pos_audio, audio), 1)\n",
    "                            elif label == 'Negative':\n",
    "                                if self.neg_audio is None:\n",
    "                                    self.neg_audio = get_file(os.path.join(root, file))\n",
    "                                elif torch.is_tensor(self.neg_audio):\n",
    "                                    audio = get_file(os.path.join(root, file))\n",
    "                                    self.neg_audio = torch.cat((self.neg_audio, audio), 1)\n",
    "                        except :\n",
    "                            print(f'Error loading {os.path.join(root, file)}')\n",
    "\n",
    "        print('Finished loading audio into memory')\n",
    "        self.pos_audio_len = int(math.ceil(self.pos_audio.shape[1] / (self.chunk_step * self.sr))\n",
    "                                 - (self.chunk_size / self.chunk_step))\n",
    "        self.neg_audio_len = int(math.ceil(self.neg_audio.shape[1] / (self.chunk_step * self.sr))\n",
    "                                 - (self.chunk_size / self.chunk_step))\n",
    "        \n",
    "        print('Generating positive spectogram')\n",
    "        self.pos_spectogram = self.get_spectogram(self.pos_audio)\n",
    "        if not self.keep_audio:\n",
    "            self.pos_audio = None\n",
    "        print('Generating negative spectogram')\n",
    "        self.neg_spectogram = self.get_spectogram(self.neg_audio)\n",
    "        if not self.keep_audio:\n",
    "            self.neg_audio = None\n",
    "        \n",
    "        #total_spectogram = torch.cat((self.pos_spectogram, self.neg_spectogram), 2)\n",
    "        #self.spec_mean = torch.mean(total_spectogram)\n",
    "        #self.spec_std = torch.std(total_spectogram)\n",
    "        #total_spectogram = None\n",
    "        #print(f'Dataset mean = {self.spec_mean:.4f} std = {self.spec_std:.4f}')\n",
    "        \n",
    "        # hard coding mean and std for my dataset\n",
    "        if use_mel:\n",
    "            self.spec_mean = -16.5 # mel spectogram\n",
    "            self.spec_std = 14.5 # mel spectogram\n",
    "        else:\n",
    "            self.spec_mean = -30 # spectogram\n",
    "            self.spec_std = 15 # spectogram\n",
    "        \n",
    "        self.pos_spectogram = (self.pos_spectogram - self.spec_mean) / self.spec_std\n",
    "        self.neg_spectogram = (self.neg_spectogram - self.spec_mean) / self.spec_std\n",
    "    \n",
    "    def get_spectogram(self, audio):\n",
    "        if self.use_mel:\n",
    "            spectogram = torchaudio.transforms.MelSpectrogram(sample_rate = self.sr,\n",
    "                                                                n_fft = self.frame_length,\n",
    "                                                                win_length = self.win_length,\n",
    "                                                                hop_length = self.hop_length,\n",
    "                                                                n_mels = self.mel_count,\n",
    "                                                                window_fn = torch.hamming_window)(audio)\n",
    "        else:  \n",
    "            window = torch.hamming_window(self.win_length, requires_grad=False)\n",
    "            spectogram = torchaudio.functional.spectrogram(audio,\n",
    "                                                            pad = 0,\n",
    "                                                            n_fft = self.frame_length,\n",
    "                                                            win_length = self.win_length,\n",
    "                                                            hop_length = self.hop_length,\n",
    "                                                            window = window,\n",
    "                                                            power = 2,\n",
    "                                                            normalized = False)\n",
    "            \n",
    "        spectogram = torchaudio.functional.amplitude_to_DB(spectogram,\n",
    "                                                            amin = 1e-10,\n",
    "                                                            multiplier = 10.0,\n",
    "                                                            db_multiplier = 0)\n",
    "        \n",
    "        return spectogram\n",
    "    \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.pos_audio_len + self.neg_audio_len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "        wobble = random.randint(0,self.wobble)\n",
    "        \n",
    "        if(idx >= self.pos_audio_len):\n",
    "            start = math.ceil((idx - self.pos_audio_len) * self.chunk_step * self.sr)\n",
    "            if self.use_wobble:\n",
    "                start += wobble * self.hop_length\n",
    "            if self.keep_audio:\n",
    "                end = math.ceil(start + self.chunk_size * self.sr)\n",
    "                audio = self.neg_audio[:,start:end]\n",
    "\n",
    "            start_spectogram = math.floor(start / self.hop_length)\n",
    "            end_spectogram = math.floor(start_spectogram + (self.chunk_size * self.sr) / self.hop_length)\n",
    "            spectogram = self.neg_spectogram[:,:,start_spectogram:end_spectogram]\n",
    "\n",
    "            label = 0.\n",
    "        else:\n",
    "            start = math.ceil(idx * self.chunk_step * self.sr)\n",
    "            if self.use_wobble:\n",
    "                start += wobble * self.hop_length\n",
    "            if self.keep_audio:\n",
    "                end = math.ceil(start + self.chunk_size * self.sr)\n",
    "                audio = self.pos_audio[:,start:end]\n",
    "            \n",
    "            start_spectogram = math.floor(start / self.hop_length)\n",
    "            end_spectogram = math.floor(start_spectogram + (self.chunk_size * self.sr) / self.hop_length)\n",
    "            spectogram = self.pos_spectogram[:,:,start_spectogram:end_spectogram]\n",
    "\n",
    "            label = 1.\n",
    "        if self.keep_audio:\n",
    "            audio = audio.squeeze(0)\n",
    "        else:\n",
    "            audio = torch.tensor([0])\n",
    "        spectogram = spectogram.squeeze(0)\n",
    "\n",
    "        if self.keep_audio:\n",
    "            if audio.shape[0] < self.chunk_size * self.sr:\n",
    "                pad = torch.zeros(self.chunk_size * self.sr)\n",
    "                pad[:audio.shape[0]] = audio\n",
    "                audio = pad\n",
    "        spec_length = math.floor((self.chunk_size * self.sr) / self.hop_length)\n",
    "        if spectogram.shape[1] < spec_length:\n",
    "            pad = torch.zeros(spectogram.shape[0], spec_length)\n",
    "            pad[:,:spectogram.shape[1]] = spectogram\n",
    "            spectogram = pad\n",
    "        \n",
    "        return spectogram, audio, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_outputs, dropout = 0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        # x -> batch_size, sequence_length, input_size\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_outputs)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(DEVICE)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(DEVICE)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # out: batch_size, sequence_length, hidden_size\n",
    "        # out: (N, 88, 1024)\n",
    "        out = out[:, -1, :]\n",
    "        # out (N, 1024) i.e. Take the last in the sequence only\n",
    "        out = self.batchnorm(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "chunk_size = 3 # seconds\n",
    "chunk_step = 1/40 # seconds\n",
    "use_wobble = True\n",
    "wobble = 1 # wobbles the dataset by 0 or 1 hop randomly (no point increasing this unless chunk_step is reduced)\n",
    "sr = 22050 # Hz\n",
    "frame_length = 1024 # samples\n",
    "win_length = 1024 # samples\n",
    "hop_length = 512 # samples\n",
    "use_mel = True\n",
    "mel_count = 128\n",
    "if use_mel:\n",
    "    input_size = mel_count\n",
    "else:\n",
    "    input_size = math.ceil((frame_length / 2) + 1)\n",
    "hidden_size = 1024\n",
    "sequence_length = math.floor((chunk_size * sr) / hop_length)\n",
    "num_layers = 2\n",
    "num_outputs = 1\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.15\n",
    "threshold = 0.99\n",
    "model_file = 'model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss function and optimiser\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_outputs, dropout).to(DEVICE)\n",
    "if os.path.isfile(model_file):\n",
    "    try:\n",
    "        state_dict = torch.load(model_file)\n",
    "        model.load_state_dict(state_dict) ## Will this work?\n",
    "    except:\n",
    "        print('Failed to load saved model')\n",
    "print(model)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test sets\n",
    "train_dataset = None # frees up some memory when re-running this cell\n",
    "test_dataset = None # frees up some memory when re-running this cell\n",
    "train_dataset = AudioDataset('Data',\n",
    "                             dataset = 'train',\n",
    "                             chunk_size = chunk_size,\n",
    "                             chunk_step = chunk_step,\n",
    "                             sr = sr,\n",
    "                             frame_length = frame_length,\n",
    "                             win_length = win_length,\n",
    "                             hop_length = hop_length,\n",
    "                             use_mel = use_mel,\n",
    "                             mel_count = mel_count,\n",
    "                             keep_audio = True,\n",
    "                             use_wobble = use_wobble,\n",
    "                             wobble = wobble)\n",
    "\n",
    "test_dataset = AudioDataset('Data',\n",
    "                             dataset = 'test',\n",
    "                             chunk_size = chunk_size,\n",
    "                             chunk_step = chunk_step,\n",
    "                             sr = sr,\n",
    "                             frame_length = frame_length,\n",
    "                             win_length = win_length,\n",
    "                             hop_length = hop_length,\n",
    "                             use_mel = use_mel,\n",
    "                             mel_count = mel_count,\n",
    "                             keep_audio = True,\n",
    "                             use_wobble = use_wobble,\n",
    "                             wobble = wobble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def analyse_predictions(title, predictions, labels, audio, spectograms, outputs,\n",
    "                        show_false_negatives = False, show_false_positives = True):\n",
    "        correct = torch.sum(predictions == labels)\n",
    "        total = labels.shape[0]\n",
    "        true_pos = (predictions == labels) * labels\n",
    "        true_neg = (predictions == labels) * -(labels-1)\n",
    "        false_pos = (predictions != labels) * -(labels-1)\n",
    "        false_neg = (predictions != labels) * labels\n",
    "        true_pos_sum = torch.sum(true_pos)\n",
    "        true_neg_sum = torch.sum(true_neg)\n",
    "        false_pos_sum = torch.sum(false_pos)\n",
    "        false_neg_sum = torch.sum(false_neg)\n",
    "        false_pos_index = false_pos.nonzero(as_tuple=False)\n",
    "        false_neg_index = false_neg.nonzero(as_tuple=False)\n",
    "        accuracy = correct / total\n",
    "        print(title)\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'True Positive: {true_pos_sum}')\n",
    "        print(f'True Negative: {true_neg_sum}')\n",
    "        print(f'False Positive: {false_pos_sum}')\n",
    "        if show_false_positives:\n",
    "            for idx in false_pos_index:\n",
    "                print('False Positive Example:')\n",
    "                print(f'Score: {outputs[idx[0]].item()}')\n",
    "                spect = spectograms[idx[0]].squeeze(0).to('cpu')\n",
    "                plt.imshow(spect, cmap='rainbow_r')\n",
    "                plt.show()\n",
    "                if torch.is_tensor(audio[idx[0]]):\n",
    "                    IPython.display.display(Audio(data=audio[idx[0]], rate=sr))\n",
    "        print(f'False Negative: {false_neg_sum}')\n",
    "        if show_false_negatives:\n",
    "            for idx in false_neg_index:\n",
    "                print('False Negative Example:')\n",
    "                print(f'Score: {outputs[idx[0]].item()}')\n",
    "                spect = spectograms[idx[0]].squeeze(0).to('cpu')\n",
    "                plt.imshow(spect, cmap='rainbow_r')\n",
    "                plt.show()\n",
    "                IPython.display.display(Audio(data=audio[idx[0]], rate=sr))\n",
    "\n",
    "model.train()\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (spectograms, audio, labels) in enumerate(train_loader):  \n",
    "        spectograms = spectograms.reshape(-1, sequence_length, input_size).to(DEVICE)\n",
    "        labels = torch.unsqueeze(labels,1).float().to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(spectograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('=======================================================================================')\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.12f}')\n",
    "            if (i+1) % 1000 == 0:\n",
    "                \n",
    "                # How are we doing against a batch of the training set?\n",
    "                model.eval()\n",
    "                (spectograms, audio, labels) = next(iter(train_loader))\n",
    "                spect_plots = spectograms\n",
    "                spectograms = spectograms.reshape(-1, sequence_length, input_size).to(DEVICE)\n",
    "                labels = torch.unsqueeze(labels,1).float().to(DEVICE)\n",
    "                outputs = model(spectograms)\n",
    "                predictions = (outputs>threshold).float()\n",
    "                analyse_predictions('TRAIN SET', predictions, labels, audio, spect_plots, outputs,\n",
    "                                    show_false_positives = True, show_false_negatives = True)\n",
    "\n",
    "                # How are we doing against a batch of the test set?\n",
    "                (spectograms, audio, labels) = next(iter(test_loader))\n",
    "                spect_plots = spectograms\n",
    "                spectograms = spectograms.reshape(-1, sequence_length, input_size).to(DEVICE)\n",
    "                labels = torch.unsqueeze(labels,1).float().to(DEVICE)\n",
    "                outputs = model(spectograms)\n",
    "                predictions = (outputs>threshold).float()\n",
    "                analyse_predictions('TEST SET', predictions, labels, audio, spect_plots, outputs,\n",
    "                                   show_false_positives = True, show_false_negatives = False)\n",
    "                model.train()\n",
    "\n",
    "                # save model state\n",
    "                torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
